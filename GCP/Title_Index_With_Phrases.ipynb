{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Index with Pre-computed Phrases\n",
    "\n",
    "**Important: DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!**\n",
    "\n",
    "This notebook builds a Title inverted index using:\n",
    "- Pre-computed phrases from `gs://db204905756/phrases/strong_phrases.pkl`\n",
    "- Pre-computed title doc lengths from `gs://db204905756/title_stemmed/title_doc_lengths.pickle`\n",
    "\n",
    "**Output folder:** `title_postingsPhrases_gcp/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import hashlib\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check graphframes jar\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket configuration\n",
    "bucket_name = 'db204905756'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords & Stemmer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Porter Stemmer\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\"-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data - TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Select TITLE (not body text!)\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "\n",
    "print(f\"Total documents: {parquetFile.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample\n",
    "sample = doc_title_pairs.take(5)\n",
    "for s in sample:\n",
    "    print(f\"ID: {s[1]}, Title: {s[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-computed Phrases from GCS\n",
    "\n",
    "**No need to calculate PMI - we use existing phrases!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download strong_phrases.pkl from GCS\n",
    "print(\"Loading pre-computed phrases from GCS...\")\n",
    "!gsutil cp gs://db204905756/phrases/strong_phrases.pkl strong_phrases.pkl\n",
    "\n",
    "# Load the phrases\n",
    "with open('strong_phrases.pkl', 'rb') as f:\n",
    "    strong_phrases = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(strong_phrases):,} pre-computed phrases!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample phrases:\")\n",
    "for i, phrase in enumerate(list(strong_phrases)[:10]):\n",
    "    print(f\"  {phrase[0]}_{phrase[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-computed Title Doc Lengths\n",
    "\n",
    "**No need to recalculate - we use existing doc lengths!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing title doc lengths\n",
    "print(\"Loading pre-computed title doc lengths from GCS...\")\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob('title_stemmed/title_doc_lengths.pickle')\n",
    "contents = blob.download_as_bytes()\n",
    "title_doc_lengths_dict = pickle.loads(contents)\n",
    "\n",
    "print(f\"✅ Loaded {len(title_doc_lengths_dict):,} title doc lengths\")\n",
    "\n",
    "# Calculate average\n",
    "avg_title_len = sum(title_doc_lengths_dict.values()) / len(title_doc_lengths_dict)\n",
    "print(f\"Average title length: {avg_title_len:.2f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Function (with Stemming + Phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_phrases_stemmed(text, phrases_set, stopwords_set):\n",
    "    \"\"\"\n",
    "    Tokenize text with stemming, replacing recognized phrases with single tokens.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    # Extract and stem tokens\n",
    "    raw_tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    tokens = [STEMMER.stem(t) for t in raw_tokens if t not in stopwords_set]\n",
    "    \n",
    "    if len(tokens) <= 1:\n",
    "        return tokens\n",
    "    \n",
    "    # Merge phrases\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            if bigram in phrases_set:\n",
    "                result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                i += 2\n",
    "                continue\n",
    "        result.append(tokens[i])\n",
    "        i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test\n",
    "test_title = \"United States President\"\n",
    "test_result = tokenize_with_phrases_stemmed(test_title, strong_phrases, all_stopwords)\n",
    "print(f\"Input: {test_title}\")\n",
    "print(f\"Output: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom InvertedIndex with Configurable Folder\n",
    "\n",
    "We define our own write functions to use `title_postingsPhrases_gcp/` folder instead of the default `postings_gcp/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "POSTINGS_FOLDER = \"title_postingsPhrases_gcp\"  # <-- Custom folder name!\n",
    "BLOCK_SIZE = 1999998\n",
    "TUPLE_SIZE = 6\n",
    "TF_MASK = 2 ** 16 - 1\n",
    "\n",
    "print(f\"✅ Posting lists will be written to: gs://{bucket_name}/{POSTINGS_FOLDER}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFileWriter:\n",
    "    \"\"\"Sequential binary writer to multiple files - writes to custom folder\"\"\"\n",
    "    def __init__(self, base_dir, name, bucket_name, folder_name):\n",
    "        self._base_dir = Path(base_dir)\n",
    "        self._name = name\n",
    "        self._folder_name = folder_name\n",
    "        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n",
    "                          for i in itertools.count())\n",
    "        self._f = next(self._file_gen)\n",
    "        self.client = storage.Client()\n",
    "        self.bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "    def write(self, b):\n",
    "        locs = []\n",
    "        while len(b) > 0:\n",
    "            pos = self._f.tell()\n",
    "            remaining = BLOCK_SIZE - pos\n",
    "            if remaining == 0:  \n",
    "                self._f.close()\n",
    "                self.upload_to_gcp()                \n",
    "                self._f = next(self._file_gen)\n",
    "                pos, remaining = 0, BLOCK_SIZE\n",
    "            self._f.write(b[:remaining])\n",
    "            locs.append((self._f.name, pos))\n",
    "            b = b[remaining:]\n",
    "        return locs\n",
    "\n",
    "    def close(self):\n",
    "        self._f.close()\n",
    "    \n",
    "    def upload_to_gcp(self):\n",
    "        file_name = self._f.name\n",
    "        # Use custom folder name!\n",
    "        blob = self.bucket.blob(f\"{self._folder_name}/{file_name}\")\n",
    "        blob.upload_from_filename(file_name)\n",
    "\n",
    "\n",
    "def write_a_posting_list(b_w_pl, bucket_name, folder_name):\n",
    "    \"\"\"Write posting list to GCS with custom folder\"\"\"\n",
    "    posting_locs = defaultdict(list)\n",
    "    bucket_id, list_w_pl = b_w_pl\n",
    "    \n",
    "    with closing(MultiFileWriter(\".\", bucket_id, bucket_name, folder_name)) as writer:\n",
    "        for w, pl in list_w_pl: \n",
    "            b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
    "                          for doc_id, tf in pl])\n",
    "            locs = writer.write(b)\n",
    "            posting_locs[w].extend(locs)\n",
    "        writer.upload_to_gcp()\n",
    "        \n",
    "        # Upload posting locations pickle to custom folder\n",
    "        with open(f\"{bucket_id}_posting_locs.pickle\", \"wb\") as f:\n",
    "            pickle.dump(dict(posting_locs), f)\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(f\"{folder_name}/{bucket_id}_posting_locs.pickle\")\n",
    "        blob.upload_from_filename(f\"{bucket_id}_posting_locs.pickle\")\n",
    "        \n",
    "    return bucket_id\n",
    "\n",
    "print(\"✅ Custom MultiFileWriter defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# Broadcast phrases and stopwords to all workers\n",
    "strong_phrases_broadcast = sc.broadcast(strong_phrases)\n",
    "all_stopwords_broadcast = sc.broadcast(all_stopwords)\n",
    "\n",
    "print(f\"✅ Broadcasted {len(strong_phrases):,} phrases to all workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Inverted Index Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"Map token to bucket number\"\"\"\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count_with_phrases_stemmed(title, doc_id):\n",
    "    \"\"\"\n",
    "    Count term frequency for each stemmed token (including phrases) in title.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_with_phrases_stemmed(\n",
    "        title, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    counts = Counter(tokens)\n",
    "    return [(token, (doc_id, tf)) for token, tf in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Sort posting list by doc_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each token\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings):\n",
    "    \"\"\"Partition and write posting lists to GCS - writes to title_postingsPhrases_gcp folder\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), \n",
    "            bucket_name, \n",
    "            POSTINGS_FOLDER  # <-- Use custom folder!\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)\n",
    "\n",
    "print(f\"✅ Functions defined - posting lists will be written to: {POSTINGS_FOLDER}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Title Index\n",
    "\n",
    "**This should be fast since titles are short (~30-60 minutes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Building inverted index with stemming and phrases for TITLES...\")\n",
    "t_start = time()\n",
    "\n",
    "# Step 1: Word counts (stemmed + phrases)\n",
    "word_counts = doc_title_pairs.flatMap(lambda x: word_count_with_phrases_stemmed(x[0], x[1]))\n",
    "\n",
    "# Step 2: Create posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Step 3: Filter rare terms (optional for titles, use lower threshold)\n",
    "MIN_DF = 3  # Lower threshold for titles since they're short\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) >= MIN_DF)\n",
    "\n",
    "# Step 4: Calculate df\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"\\n✅ Total unique tokens (with df >= {MIN_DF}): {len(w2df_dict):,}\")\n",
    "print(f\"Time: {(time() - t_start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Write posting lists to GCS\n",
    "print(f\"Writing posting lists to {POSTINGS_FOLDER}/...\")\n",
    "_ = partition_postings_and_write(postings_filtered).collect()\n",
    "print(\"✅ Done writing posting lists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Posting Locations & Save Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all posting list locations from our custom folder\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix=POSTINGS_FOLDER):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "print(f\"✅ Collected posting locations for {len(super_posting_locs):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create InvertedIndex class for saving\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.df = {}\n",
    "        self.posting_locs = {}\n",
    "    \n",
    "    def write_index(self, base_dir, name):\n",
    "        with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "# Create and save InvertedIndex\n",
    "inverted = InvertedIndex()\n",
    "inverted.posting_locs = dict(super_posting_locs)\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# Save locally\n",
    "inverted.write_index('.', 'index')\n",
    "\n",
    "# Upload to GCS - to the index folder\n",
    "index_src = \"index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/title_stemmed_phrases_idx/index.pkl'\n",
    "!gsutil cp $index_src $index_dst\n",
    "\n",
    "print(f\"\\n✅ Index saved to {index_dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy title doc lengths to the new index folder\n",
    "!gsutil cp gs://$bucket_name/title_stemmed/title_doc_lengths.pickle gs://$bucket_name/title_stemmed_phrases_idx/title_doc_lengths.pickle\n",
    "\n",
    "# Copy phrases to the new index folder\n",
    "!gsutil cp gs://$bucket_name/phrases/strong_phrases.pkl gs://$bucket_name/title_stemmed_phrases_idx/strong_phrases.pkl\n",
    "\n",
    "print(\"✅ All supporting files copied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Files in title_stemmed_phrases_idx/:\")\n",
    "print(\"=\" * 50)\n",
    "!gsutil ls -lh gs://$bucket_name/title_stemmed_phrases_idx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count posting list files in the custom folder\n",
    "print(f\"\\nPosting list files in {POSTINGS_FOLDER}/:\")\n",
    "!gsutil ls gs://$bucket_name/title_postingsPhrases_gcp/ | head -20\n",
    "print(\"...\")\n",
    "!gsutil ls gs://$bucket_name/title_postingsPhrases_gcp/ | wc -l\n",
    "print(\"total files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Created:\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| index.pkl | title_stemmed_phrases_idx/ | Inverted index (posting locs + df) |\n",
    "| title_doc_lengths.pickle | title_stemmed_phrases_idx/ | Document lengths for BM25 |\n",
    "| strong_phrases.pkl | title_stemmed_phrases_idx/ | Set of detected phrases |\n",
    "| *.bin | title_postingsPhrases_gcp/ | Binary posting list files |\n",
    "| *_posting_locs.pickle | title_postingsPhrases_gcp/ | Posting location files |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ Title Index with Phrases - COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nVocabulary size: {len(w2df_dict):,}\")\n",
    "print(f\"Documents: {len(title_doc_lengths_dict):,}\")\n",
    "print(f\"Average title length: {avg_title_len:.2f}\")\n",
    "print(f\"\\nIndex location: gs://{bucket_name}/title_stemmed_phrases_idx/\")\n",
    "print(f\"Posting lists: gs://{bucket_name}/{POSTINGS_FOLDER}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
