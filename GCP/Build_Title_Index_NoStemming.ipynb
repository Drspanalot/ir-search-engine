{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Title Index (No Stemming)\n",
    "\n",
    "**Important: DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!**\n",
    "\n",
    "This notebook builds an inverted index for Wikipedia article **titles** WITHOUT stemming.\n",
    "\n",
    "This index is used for the `search_title` endpoint which:\n",
    "- Returns ALL search results containing query words in titles\n",
    "- Orders by NUMBER OF DISTINCT QUERY WORDS in the title\n",
    "- Does NOT use stemming\n",
    "- Uses the staff-provided tokenizer to tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cluster is running\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify graphframes jar is available\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket name - CHANGE THIS TO YOUR BUCKET\n",
    "bucket_name = 'db204905756'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of wiki pages - should be more than 6M\n",
    "print(f\"Total documents: {parquetFile.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Modified Inverted Index Module (for title_nostem folder)\n",
    "\n",
    "We need to modify the inverted index module to save postings to a different folder (`postings_title_nostem/`) instead of the default `postings_gcp/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/dataproc/inverted_index_title_nostem.py\n",
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from collections import defaultdict\n",
    "from contextlib import closing\n",
    "\n",
    "# Block size for posting files\n",
    "BLOCK_SIZE = 1999998\n",
    "\n",
    "# Folder name for this index's postings\n",
    "POSTINGS_FOLDER = \"postings_title_nostem\"\n",
    "\n",
    "class MultiFileWriter:\n",
    "    \"\"\" Sequential binary writer to multiple files of up to BLOCK_SIZE each. \"\"\"\n",
    "    def __init__(self, base_dir, name, bucket_name):\n",
    "        self._base_dir = Path(base_dir)\n",
    "        self._name = name\n",
    "        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n",
    "                          for i in itertools.count())\n",
    "        self._f = next(self._file_gen)\n",
    "        # Connecting to google storage bucket. \n",
    "        self.client = storage.Client()\n",
    "        self.bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "    \n",
    "    def write(self, b):\n",
    "        locs = []\n",
    "        while len(b) > 0:\n",
    "            pos = self._f.tell()\n",
    "            remaining = BLOCK_SIZE - pos\n",
    "        # if the current file is full, close and open a new one.\n",
    "            if remaining == 0:  \n",
    "                self._f.close()\n",
    "                self.upload_to_gcp()                \n",
    "                self._f = next(self._file_gen)\n",
    "                pos, remaining = 0, BLOCK_SIZE\n",
    "            self._f.write(b[:remaining])\n",
    "            locs.append((self._f.name, pos))\n",
    "            b = b[remaining:]\n",
    "        return locs\n",
    "\n",
    "    def close(self):\n",
    "        self._f.close()\n",
    "    \n",
    "    def upload_to_gcp(self):\n",
    "        '''\n",
    "            The function saves the posting files into the right bucket in google storage.\n",
    "        '''\n",
    "        file_name = self._f.name\n",
    "        blob = self.bucket.blob(f\"{POSTINGS_FOLDER}/{file_name}\")\n",
    "        blob.upload_from_filename(file_name)\n",
    "\n",
    "        \n",
    "\n",
    "class MultiFileReader:\n",
    "    \"\"\" Sequential binary reader of multiple files of up to BLOCK_SIZE each. \"\"\"\n",
    "    def __init__(self):\n",
    "        self._open_files = {}\n",
    "\n",
    "    def read(self, locs, n_bytes):\n",
    "        b = []\n",
    "        for f_name, offset in locs:\n",
    "            if f_name not in self._open_files:\n",
    "                self._open_files[f_name] = open(f_name, 'rb')\n",
    "            f = self._open_files[f_name]\n",
    "            f.seek(offset)\n",
    "            n_read = min(n_bytes, BLOCK_SIZE - offset)\n",
    "            b.append(f.read(n_read))\n",
    "            n_bytes -= n_read\n",
    "        return b''.join(b)\n",
    "  \n",
    "    def close(self):\n",
    "        for f in self._open_files.values():\n",
    "            f.close()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.close()\n",
    "        return False \n",
    "\n",
    "\n",
    "TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this many bytes.\n",
    "TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer\n",
    "\n",
    "\n",
    "class InvertedIndex:  \n",
    "    def __init__(self, docs={}):\n",
    "        \"\"\" Initializes the inverted index and add documents to it (if provided).\n",
    "        Parameters:\n",
    "        -----------\n",
    "          docs: dict mapping doc_id to list of tokens\n",
    "        \"\"\"\n",
    "        # stores document frequency per term\n",
    "        self.df = Counter()\n",
    "        # stores total frequency per term\n",
    "        self.term_total = Counter()\n",
    "        # stores posting list per term while building the index (internally), \n",
    "        # otherwise too big to store in memory.\n",
    "        self._posting_list = defaultdict(list)\n",
    "        # mapping a term to posting file locations\n",
    "        self.posting_locs = defaultdict(list)\n",
    "\n",
    "        for doc_id, tokens in docs.items():\n",
    "            self.add_doc(doc_id, tokens)\n",
    "\n",
    "    def add_doc(self, doc_id, tokens):\n",
    "        \"\"\" Adds a document to the index with a given `doc_id` and tokens. \"\"\"\n",
    "        w2cnt = Counter(tokens)\n",
    "        self.term_total.update(w2cnt)\n",
    "        for w, cnt in w2cnt.items():\n",
    "            self.df[w] = self.df.get(w, 0) + 1\n",
    "            self._posting_list[w].append((doc_id, cnt))\n",
    "\n",
    "    def write_index(self, base_dir, name):\n",
    "        \"\"\" Write the in-memory index to disk. \"\"\"\n",
    "        self._write_globals(base_dir, name)\n",
    "\n",
    "    def _write_globals(self, base_dir, name):\n",
    "        with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\" Modify how the object is pickled by removing the internal posting lists. \"\"\"\n",
    "        state = self.__dict__.copy()\n",
    "        del state['_posting_list']\n",
    "        return state\n",
    "\n",
    "    def posting_lists_iter(self):\n",
    "        \"\"\" A generator that reads one posting list from disk and yields \n",
    "            a (word:str, [(doc_id:int, tf:int), ...]) tuple.\n",
    "        \"\"\"\n",
    "        with closing(MultiFileReader()) as reader:\n",
    "            for w, locs in self.posting_locs.items():\n",
    "                b = reader.read(locs[0], self.df[w] * TUPLE_SIZE)\n",
    "                posting_list = []\n",
    "                for i in range(self.df[w]):\n",
    "                    doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')\n",
    "                    tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')\n",
    "                    posting_list.append((doc_id, tf))\n",
    "                yield w, posting_list\n",
    "\n",
    "    @staticmethod\n",
    "    def read_index(base_dir, name):\n",
    "        with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_index(base_dir, name):\n",
    "        path_globals = Path(base_dir) / f'{name}.pkl'\n",
    "        path_globals.unlink()\n",
    "        for p in Path(base_dir).rglob(f'{name}_*.bin'):\n",
    "            p.unlink()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def write_a_posting_list(b_w_pl, bucket_name):\n",
    "        posting_locs = defaultdict(list)\n",
    "        bucket_id, list_w_pl = b_w_pl\n",
    "        \n",
    "        with closing(MultiFileWriter(\".\", bucket_id, bucket_name)) as writer:\n",
    "            for w, pl in list_w_pl: \n",
    "                # convert to bytes\n",
    "                b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
    "                              for doc_id, tf in pl])\n",
    "                # write to file(s)\n",
    "                locs = writer.write(b)\n",
    "                # save file locations to index\n",
    "                posting_locs[w].extend(locs)\n",
    "            writer.upload_to_gcp() \n",
    "            InvertedIndex._upload_posting_locs(bucket_id, posting_locs, bucket_name)\n",
    "        return bucket_id\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _upload_posting_locs(bucket_id, posting_locs, bucket_name):\n",
    "        with open(f\"{bucket_id}_posting_locs.pickle\", \"wb\") as f:\n",
    "            pickle.dump(posting_locs, f)\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob_posting_locs = bucket.blob(f\"{POSTINGS_FOLDER}/{bucket_id}_posting_locs.pickle\")\n",
    "        blob_posting_locs.upload_from_filename(f\"{bucket_id}_posting_locs.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file was created\n",
    "!ls -l /home/dataproc/inverted_index_title_nostem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the modified module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_title_nostem.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "from inverted_index_title_nostem import InvertedIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenization (NO STEMMING)\n",
    "\n",
    "Following the assignment requirements:\n",
    "- Use the staff-provided tokenizer regex\n",
    "- Remove stopwords\n",
    "- **DO NOT use stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords setup\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = ['category', 'references', 'also\"', 'links', 'extrenal',\n",
    "                 'first', 'see', 'new', 'two', 'list', 'may', 'one', 'district',\n",
    "                 'including', 'became', 'however', 'com', 'many', 'began',\n",
    "                 'make', 'made', 'part', 'would', 'people', 'second', 'also',\n",
    "                 'following', 'history', 'thumb', 'external']\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "\n",
    "# Staff-provided tokenizer regex\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\'\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "# Number of buckets for partitioning\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_stem(text):\n",
    "    \"\"\"\n",
    "    Tokenize text WITHOUT stemming.\n",
    "    - Uses staff-provided regex tokenizer\n",
    "    - Converts to lowercase\n",
    "    - Removes stopwords\n",
    "    - Does NOT apply stemming\n",
    "    \"\"\"\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    # Filter out stopwords but DO NOT stem\n",
    "    filtered = [token for token in tokens if token not in all_stopwords]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def word_count_no_stem(text, doc_id):\n",
    "    \"\"\"\n",
    "    Count word occurrences in a document (title) without stemming.\n",
    "    Returns list of (token, (doc_id, count)) tuples.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_no_stem(text)\n",
    "    token_counts = Counter(tokens)\n",
    "    return [(token, (doc_id, count)) for token, count in token_counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Sort posting list by doc_id.\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each term.\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    \"\"\"\n",
    "    Partition posting lists by bucket and write to GCS.\n",
    "    \"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "\n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return InvertedIndex.write_a_posting_list((bucket_id, list(word_posting_pairs)), bucket_name)\n",
    "\n",
    "    posting_locs_list = bucket_rdd.map(write_bucket)\n",
    "    return posting_locs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the index creation\n",
    "start_time = time()\n",
    "\n",
    "# Word counts map - using NO STEMMING\n",
    "# doc_title_pairs has format: Row(title='...', id=...)\n",
    "word_counts = doc_title_pairs.flatMap(lambda x: word_count_no_stem(x[0], x[1]))\n",
    "\n",
    "# Group by word and create posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Calculate document frequencies\n",
    "w2df = calculate_df(postings)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"Number of unique terms: {len(w2df_dict)}\")\n",
    "print(f\"Time for word counting and DF calculation: {time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition posting lists and write to GCS\n",
    "write_start = time()\n",
    "_ = partition_postings_and_write(postings, \"title_nostem\").collect()\n",
    "print(f\"Time for writing postings: {time() - write_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all posting list locations into one dictionary\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix='postings_title_nostem'):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "print(f\"Collected posting locations for {len(super_posting_locs)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index instance\n",
    "inverted = InvertedIndex()\n",
    "\n",
    "# Add posting locations and document frequencies\n",
    "inverted.posting_locs = super_posting_locs\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# Write the index locally\n",
    "inverted.write_index('.', 'index')\n",
    "\n",
    "# Upload to GCS - to a NEW directory 'title_nostem'\n",
    "index_src = \"index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/title_nostem/{index_src}'\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the upload\n",
    "!gsutil ls -lh $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Lengths (for potential TF-IDF use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_tokens_no_stem(doc_id, text):\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a title (no stemming).\n",
    "    \"\"\"\n",
    "    tokens = tokenize_no_stem(text)\n",
    "    return (doc_id, len(tokens))\n",
    "\n",
    "# Calculate document lengths\n",
    "# Note: doc_title_pairs has Row(title, id) format\n",
    "title_len_pairs = doc_title_pairs.map(lambda x: len_tokens_no_stem(x[1], x[0]))\n",
    "title_doc_lengths_dict = title_len_pairs.collectAsMap()\n",
    "\n",
    "print(f\"Calculated lengths for {len(title_doc_lengths_dict)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document lengths\n",
    "with open('title_doc_lengths.pickle', 'wb') as f:\n",
    "    pickle.dump(title_doc_lengths_dict, f)\n",
    "\n",
    "src = \"title_doc_lengths.pickle\"\n",
    "dest = f'gs://{bucket_name}/title_nostem/{src}'\n",
    "!gsutil cp $src $dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "!gsutil ls -lh gs://{bucket_name}/title_nostem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook created the following files in GCS:\n",
    "\n",
    "1. **`gs://{bucket_name}/title_nostem/index.pkl`** - The inverted index containing:\n",
    "   - `posting_locs`: Dictionary mapping terms to their posting list file locations\n",
    "   - `df`: Document frequency dictionary\n",
    "\n",
    "2. **`gs://{bucket_name}/title_nostem/title_doc_lengths.pickle`** - Document lengths dictionary\n",
    "\n",
    "3. **`gs://{bucket_name}/postings_title_nostem/`** - Posting list binary files (124 buckets)\n",
    "\n",
    "### Key Differences from Stemmed Index:\n",
    "- **NO stemming** applied to tokens\n",
    "- Stored in separate directory (`title_nostem/` instead of `title_stemmed/`)\n",
    "- Posting files in `postings_title_nostem/` instead of `postings_gcp/`\n",
    "\n",
    "### Usage in search_title:\n",
    "When using this index for the `search_title` endpoint:\n",
    "1. Tokenize query using `tokenize_no_stem()` (same function)\n",
    "2. Look up each query term in the index\n",
    "3. For each document, count the number of DISTINCT query terms that appear\n",
    "4. Return ALL matching documents, ordered by distinct query term count (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final timing\n",
    "print(f\"Total index creation time: {time() - start_time:.2f} seconds\")\n",
    "print(f\"Total unique terms: {len(w2df_dict)}\")\n",
    "print(f\"Total documents: {len(title_doc_lengths_dict)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
