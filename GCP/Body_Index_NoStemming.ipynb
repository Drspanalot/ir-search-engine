{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Text Inverted Index - NO STEMMING\n",
    "**For `search_body()` endpoint**\n",
    "\n",
    "**Important: DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!**\n",
    "\n",
    "Requirements:\n",
    "- ❌ NO Stemming\n",
    "- ❌ NO Phrases\n",
    "- ✅ Body text\n",
    "- ✅ TF-IDF ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check graphframes jar\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# IMPORTANT: Change bucket_name to your bucket!\n",
    "# ==================================\n",
    "bucket_name = 'db204905756'  # <-- שנה לשם ה-bucket שלך\n",
    "\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Setup (NO STEMMING!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ NO STEMMER - This is intentional for search_body() requirement!\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\", \"make\", \"made\",\n",
    "                    \"new\", \"list\", \"district\", \"com\", \"began\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\"-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")\n",
    "print(\"⚠️ NO STEMMING - as required for search_body()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data - Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Select BODY TEXT (not title!)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "\n",
    "print(f\"Total documents: {parquetFile.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample\n",
    "sample = doc_text_pairs.take(2)\n",
    "print(f\"Sample doc ID: {sample[0][1]}\")\n",
    "print(f\"Sample text (first 500 chars): {sample[0][0][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Function (NO STEMMING!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nostem(text, stopwords_set):\n",
    "    \"\"\"\n",
    "    Tokenize text WITHOUT stemming.\n",
    "    Uses staff-provided tokenizer (RE_WORD regex).\n",
    "    \n",
    "    This matches the requirement:\n",
    "    'DO NOT use stemming. DO USE the staff-provided tokenizer'\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    # Extract tokens using staff-provided regex\n",
    "    tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    # Remove stopwords but DO NOT STEM\n",
    "    return [t for t in tokens if t not in stopwords_set]\n",
    "\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The United States of America has many running cities and universities\"\n",
    "tokens = tokenize_nostem(test_text, all_stopwords)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Output (NO stemming): {tokens}\")\n",
    "print(\"\\n✅ Notice: 'running' stays as 'running', 'cities' stays as 'cities' (no stemming)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load InvertedIndex Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InvertedIndex module\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast stopwords to all workers\n",
    "all_stopwords_broadcast = sc.broadcast(all_stopwords)\n",
    "\n",
    "print(f\"✅ Broadcasted stopwords to all workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Inverted Index Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "INDEX_DIR = \"body_nostem\"  # ✅ NEW DIRECTORY - won't overwrite existing files!\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"Map token to bucket number\"\"\"\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count_nostem(text, doc_id):\n",
    "    \"\"\"\n",
    "    Count term frequency for each token (NO STEMMING!) in document.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_nostem(text, all_stopwords_broadcast.value)\n",
    "    counts = Counter(tokens)\n",
    "    return [(token, (doc_id, tf)) for token, tf in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Sort posting list by doc_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each token\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    \"\"\"Partition and write posting lists to GCS\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return InvertedIndex.write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), base_dir\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)\n",
    "\n",
    "print(f\"Index will be saved to: gs://{bucket_name}/{INDEX_DIR}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Building inverted index for body text WITHOUT stemming...\")\n",
    "print(\"This will take a while for the full corpus...\")\n",
    "\n",
    "# Step 1: Word counts (NO stemming!)\n",
    "word_counts = doc_text_pairs.flatMap(lambda x: word_count_nostem(x[0], x[1]))\n",
    "\n",
    "# Step 2: Create posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Step 3: Filter rare terms (helps reduce index size)\n",
    "MIN_DF = 50  # Minimum document frequency - same as stemmed version\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) >= MIN_DF)\n",
    "\n",
    "# Step 4: Calculate df\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"Total unique tokens (with df >= {MIN_DF}): {len(w2df_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Write posting lists to GCS\n",
    "print(f\"Writing posting lists to {INDEX_DIR}...\")\n",
    "_ = partition_postings_and_write(postings_filtered, INDEX_DIR).collect()\n",
    "print(\"✅ Done writing posting lists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Frequency (DF) Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save w2df dictionary\n",
    "w2df_filename = f\"{INDEX_DIR}_w2df.pkl\"\n",
    "\n",
    "with open(w2df_filename, 'wb') as f:\n",
    "    pickle.dump(w2df_dict, f)\n",
    "\n",
    "print(f\"✅ Saved w2df dictionary locally\")\n",
    "\n",
    "# Upload to GCS\n",
    "!gsutil cp {w2df_filename} gs://{bucket_name}/final_project/{w2df_filename}\n",
    "print(f\"✅ Uploaded w2df dictionary to GCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check created files in GCS\n",
    "!gsutil ls gs://{bucket_name}/{INDEX_DIR}/ | head -10\n",
    "\n",
    "# Count total files\n",
    "result = !gsutil ls gs://{bucket_name}/{INDEX_DIR}/*.pickle | wc -l\n",
    "num_files = int(result[0])\n",
    "print(f\"\\n✅ Successfully created {num_files} posting list files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Index Creation Summary ===\")\n",
    "print(f\"Total documents processed: {parquetFile.count():,}\")\n",
    "print(f\"Total unique tokens (df >= {MIN_DF}): {len(w2df_dict):,}\")\n",
    "print(f\"Index directory: {INDEX_DIR}\")\n",
    "print(f\"Number of bucket files: {NUM_BUCKETS}\")\n",
    "print(f\"\\n⚠️ STEMMING: NO (as required for search_body)\")\n",
    "\n",
    "# Show some sample frequencies - these should NOT be stemmed!\n",
    "print(\"\\nSample tokens (verify NO stemming):\")\n",
    "sample_tokens = ['united', 'states', 'running', 'cities', 'universities', \n",
    "                 'played', 'better', 'america', 'texas', 'country']\n",
    "for token in sample_tokens:\n",
    "    if token in w2df_dict:\n",
    "        print(f\"  ✅ '{token}': {w2df_dict[token]:,} documents\")\n",
    "    else:\n",
    "        print(f\"  ❌ '{token}': not found (might be filtered by MIN_DF)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Index Creation Complete!\n",
    "\n",
    "The inverted index has been successfully created with:\n",
    "- ❌ **NO Stemming** (as required for `search_body()`)\n",
    "- ❌ **NO Phrases**\n",
    "- ✅ Body text indexed\n",
    "- ✅ TF stored in posting lists (ready for TF-IDF)\n",
    "\n",
    "**Files created:**\n",
    "- `gs://{bucket_name}/body_nostem/` - Posting lists\n",
    "- `gs://{bucket_name}/final_project/body_nostem_w2df.pkl` - Document frequencies\n",
    "\n",
    "**Note:** Document lengths are already computed in the stemmed version and can be reused.\n",
    "\n",
    "**Important:** Do not clear this notebook's output!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
