{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# PMI-Based Phrase Detection for Inverted Index\n",
    "\n",
    "**מטרה:** בניית אינדקס הפוך עם תמיכה בביטויים (phrases) באמצעות PMI\n",
    "\n",
    "## מה זה PMI (Pointwise Mutual Information)?\n",
    "\n",
    "PMI מודד את הסיכוי ששתי מילים מופיעות יחד בהשוואה לסיכוי שהן מופיעות בנפרד:\n",
    "\n",
    "$$PMI(word_1, word_2) = \\log_2 \\frac{P(word_1, word_2)}{P(word_1) \\cdot P(word_2)}$$\n",
    "\n",
    "- **PMI גבוה** = המילים מופיעות יחד הרבה יותר ממה שהיינו מצפים → כנראה מושג אחד (\"new york\", \"machine learning\")\n",
    "- **PMI נמוך/שלילי** = המילים מופיעות יחד כמו שהיינו מצפים או פחות"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## שלב 1: התקנה ו-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# וודא ש-Spark עובד\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bucket-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# הגדרת ה-Bucket שלך\n",
    "bucket_name = 'hw3ir322'  # שנה לשם ה-bucket שלך!\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopwords-header",
   "metadata": {},
   "source": [
    "## שלב 2: הגדרת Stopwords ו-Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopwords",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## שלב 3: טעינת הנתונים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# טעינת כל המסמכים מה-Parquet\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# לבחירה: אינדקס על הכותרות או על הטקסט המלא\n",
    "# עבור titles:\n",
    "doc_text_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "\n",
    "# עבור text (גוף המסמך):\n",
    "# doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "\n",
    "print(f\"Total documents: {parquetFile.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pmi-header",
   "metadata": {},
   "source": [
    "## שלב 4: חישוב PMI לזיהוי ביטויים חזקים\n",
    "\n",
    "### הלוגיקה:\n",
    "1. ספירת כל המילים (unigrams)\n",
    "2. ספירת כל צמדי המילים (bigrams)\n",
    "3. חישוב PMI לכל bigram\n",
    "4. סינון רק bigrams עם PMI גבוה ותדירות מספקת"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pmi-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unigrams(text):\n",
    "    \"\"\"\n",
    "    חילוץ כל המילים (unigrams) מטקסט\n",
    "    \"\"\"\n",
    "    tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    # הסרת stopwords\n",
    "    tokens = [t for t in tokens if t not in all_stopwords]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_bigrams(text):\n",
    "    \"\"\"\n",
    "    חילוץ כל צמדי המילים הסמוכים (bigrams) מטקסט\n",
    "    \"\"\"\n",
    "    tokens = extract_unigrams(text)\n",
    "    bigrams = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigrams.append((tokens[i], tokens[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def calculate_pmi(unigram_counts, bigram_counts, total_unigrams, total_bigrams):\n",
    "    \"\"\"\n",
    "    חישוב PMI לכל bigram\n",
    "    \n",
    "    PMI(w1, w2) = log2( P(w1, w2) / (P(w1) * P(w2)) )\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    unigram_counts: dict - מספר הופעות של כל מילה\n",
    "    bigram_counts: dict - מספר הופעות של כל צמד\n",
    "    total_unigrams: int - סה\"כ מילים בקורפוס\n",
    "    total_bigrams: int - סה\"כ צמדים בקורפוס\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: bigram -> PMI score\n",
    "    \"\"\"\n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for (w1, w2), bigram_count in bigram_counts.items():\n",
    "        # P(w1, w2) - הסתברות לראות את הצמד\n",
    "        p_bigram = bigram_count / total_bigrams\n",
    "        \n",
    "        # P(w1), P(w2) - הסתברות לראות כל מילה בנפרד\n",
    "        p_w1 = unigram_counts.get(w1, 0) / total_unigrams\n",
    "        p_w2 = unigram_counts.get(w2, 0) / total_unigrams\n",
    "        \n",
    "        # מניעת חלוקה באפס\n",
    "        if p_w1 > 0 and p_w2 > 0:\n",
    "            pmi = math.log2(p_bigram / (p_w1 * p_w2))\n",
    "            pmi_scores[(w1, w2)] = pmi\n",
    "    \n",
    "    return pmi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "count-unigrams",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# שלב 4.1: ספירת Unigrams\n",
    "print(\"Counting unigrams...\")\n",
    "\n",
    "# המרה לרשימת מילים וספירה\n",
    "unigram_rdd = doc_text_pairs.flatMap(lambda x: extract_unigrams(x[0]))\n",
    "unigram_counts_rdd = unigram_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# איסוף לזיכרון\n",
    "unigram_counts = dict(unigram_counts_rdd.collect())\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "print(f\"Unique unigrams: {len(unigram_counts):,}\")\n",
    "print(f\"Total unigrams: {total_unigrams:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "count-bigrams",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# שלב 4.2: ספירת Bigrams\n",
    "print(\"Counting bigrams...\")\n",
    "\n",
    "bigram_rdd = doc_text_pairs.flatMap(lambda x: extract_bigrams(x[0]))\n",
    "bigram_counts_rdd = bigram_rdd.map(lambda bigram: (bigram, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# סינון bigrams עם תדירות מינימלית (לחסוך זיכרון)\n",
    "MIN_BIGRAM_FREQ = 5  # bigram חייב להופיע לפחות 5 פעמים\n",
    "filtered_bigram_counts_rdd = bigram_counts_rdd.filter(lambda x: x[1] >= MIN_BIGRAM_FREQ)\n",
    "\n",
    "# איסוף לזיכרון\n",
    "bigram_counts = dict(filtered_bigram_counts_rdd.collect())\n",
    "total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "print(f\"Unique bigrams (freq >= {MIN_BIGRAM_FREQ}): {len(bigram_counts):,}\")\n",
    "print(f\"Total bigrams: {total_bigrams:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculate-pmi",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# שלב 4.3: חישוב PMI\n",
    "print(\"Calculating PMI scores...\")\n",
    "\n",
    "pmi_scores = calculate_pmi(unigram_counts, bigram_counts, total_unigrams, total_bigrams)\n",
    "\n",
    "print(f\"Calculated PMI for {len(pmi_scores):,} bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-phrases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# שלב 4.4: סינון ביטויים חזקים\n",
    "\n",
    "# פרמטרים לכיול (אפשר לשחק עם הערכים)\n",
    "PMI_THRESHOLD = 5.0        # PMI מינימלי - ערך גבוה = רק ביטויים \"חזקים\" מאוד\n",
    "MIN_PHRASE_FREQ = 10       # תדירות מינימלית של הביטוי\n",
    "\n",
    "# סינון לפי PMI ותדירות\n",
    "strong_phrases = set()\n",
    "for bigram, pmi in pmi_scores.items():\n",
    "    if pmi >= PMI_THRESHOLD and bigram_counts.get(bigram, 0) >= MIN_PHRASE_FREQ:\n",
    "        strong_phrases.add(bigram)\n",
    "\n",
    "print(f\"Strong phrases found: {len(strong_phrases):,}\")\n",
    "\n",
    "# הצגת דוגמאות\n",
    "print(\"\\nTop 30 phrases by PMI:\")\n",
    "sorted_phrases = sorted(\n",
    "    [(bigram, pmi_scores[bigram], bigram_counts[bigram]) for bigram in strong_phrases],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]\n",
    "\n",
    "for bigram, pmi, freq in sorted_phrases:\n",
    "    print(f\"  {bigram[0]}_{bigram[1]}: PMI={pmi:.2f}, freq={freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-phrases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# שמירת הביטויים החזקים לקובץ pickle\n",
    "phrases_filename = 'strong_phrases.pkl'\n",
    "\n",
    "with open(phrases_filename, 'wb') as f:\n",
    "    pickle.dump(strong_phrases, f)\n",
    "\n",
    "print(f\"Saved {len(strong_phrases):,} phrases to {phrases_filename}\")\n",
    "\n",
    "# העלאה ל-GCS\n",
    "bucket = client.bucket(bucket_name)\n",
    "blob = bucket.blob(f\"phrases/{phrases_filename}\")\n",
    "blob.upload_from_filename(phrases_filename)\n",
    "\n",
    "print(f\"Uploaded to gs://{bucket_name}/phrases/{phrases_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "## שלב 5: Tokenizer עם תמיכה בביטויים\n",
    "\n",
    "הפונקציה הזו משמשת גם בבניית האינדקס וגם בזמן החיפוש"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-with-phrases",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_phrases(text, strong_phrases, stopwords):\n",
    "    \"\"\"\n",
    "    Tokenizer שמזהה ביטויים (bigrams חזקים) ומחבר אותם עם קו תחתון.\n",
    "    \n",
    "    לדוגמה: \"New York City\" → [\"new_york\", \"city\"]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: str - הטקסט לטוקניזציה\n",
    "    strong_phrases: set of tuples - סט של (word1, word2) שמייצגים ביטויים\n",
    "    stopwords: set - מילות עצירה\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list: רשימת טוקנים (כולל ביטויים מחוברים)\n",
    "    \"\"\"\n",
    "    # חילוץ כל המילים\n",
    "    all_tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    \n",
    "    # הסרת stopwords\n",
    "    tokens = [t for t in all_tokens if t not in stopwords]\n",
    "    \n",
    "    # עיבוד עם חלון נע לזיהוי ביטויים\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 1 < len(tokens):\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            if bigram in strong_phrases:\n",
    "                # מצאנו ביטוי! מחברים עם קו תחתון\n",
    "                result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                i += 2  # מדלגים על שתי המילים\n",
    "                continue\n",
    "        result.append(tokens[i])\n",
    "        i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# בדיקה\n",
    "test_text = \"New York is a great city in the United States\"\n",
    "test_result = tokenize_with_phrases(test_text, strong_phrases, all_stopwords)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Output: {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "index-header",
   "metadata": {},
   "source": [
    "## שלב 6: בניית ה-Inverted Index עם ביטויים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-index-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# טעינת המודול של InvertedIndex\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast-phrases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast של הביטויים לכל ה-Workers\n",
    "strong_phrases_broadcast = sc.broadcast(strong_phrases)\n",
    "all_stopwords_broadcast = sc.broadcast(all_stopwords)\n",
    "\n",
    "print(f\"Broadcasted {len(strong_phrases):,} phrases to all workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "index-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"ממפה טוקן למספר bucket\"\"\"\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count_with_phrases(text, doc_id):\n",
    "    \"\"\"\n",
    "    סופר תדירות של כל טוקן (כולל ביטויים) במסמך.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of tuples: [(token, (doc_id, tf)), ...]\n",
    "    \"\"\"\n",
    "    tokens = tokenize_with_phrases(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    counts = Counter(tokens)\n",
    "    return [(token, (doc_id, tf)) for token, tf in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"ממיין posting list לפי doc_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"מחשב document frequency לכל טוקן\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    \"\"\"מחלק ושומר את ה-posting lists\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return InvertedIndex.write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), base_dir, bucket_name\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# בניית האינדקס\n",
    "print(\"Building inverted index with phrases...\")\n",
    "\n",
    "# שם התיקייה באחסון (שנה לפי הצורך)\n",
    "INDEX_DIR = \"title_with_phrases\"  # או \"text_with_phrases\" לגוף הטקסט\n",
    "\n",
    "# שלב 1: ספירת מילים\n",
    "word_counts = doc_text_pairs.flatMap(lambda x: word_count_with_phrases(x[0], x[1]))\n",
    "\n",
    "# שלב 2: יצירת posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# שלב 3: חישוב df\n",
    "w2df = calculate_df(postings)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"Total unique tokens: {len(w2df_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write-postings",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# כתיבת ה-posting lists\n",
    "print(f\"Writing posting lists to {INDEX_DIR}...\")\n",
    "_ = partition_postings_and_write(postings, INDEX_DIR).collect()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-postings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# איסוף כל מיקומי ה-posting lists\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix=INDEX_DIR):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "print(f\"Collected posting locations for {len(super_posting_locs):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# יצירה ושמירה של ה-InvertedIndex\n",
    "inverted = InvertedIndex()\n",
    "inverted.posting_locs = super_posting_locs\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# שמירה מקומית\n",
    "inverted.write_index('.', 'index')\n",
    "\n",
    "# העלאה ל-GCS\n",
    "index_src = \"index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/{INDEX_DIR}_idx/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "\n",
    "print(f\"Index saved to {index_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc-lengths-header",
   "metadata": {},
   "source": [
    "## שלב 7: חישוב אורכי מסמכים (לשימוש ב-BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "doc-lengths",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doc_length(doc_id, text):\n",
    "    \"\"\"מחשב אורך מסמך בטוקנים (כולל ביטויים)\"\"\"\n",
    "    tokens = tokenize_with_phrases(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    return (doc_id, len(tokens))\n",
    "\n",
    "# חישוב אורכים\n",
    "doc_lengths_rdd = doc_text_pairs.map(lambda x: calc_doc_length(x[1], x[0]))\n",
    "doc_lengths_dict = doc_lengths_rdd.collectAsMap()\n",
    "\n",
    "print(f\"Calculated lengths for {len(doc_lengths_dict):,} documents\")\n",
    "\n",
    "# שמירה\n",
    "lengths_filename = 'doc_lengths.pickle'\n",
    "with open(lengths_filename, 'wb') as f:\n",
    "    pickle.dump(doc_lengths_dict, f)\n",
    "\n",
    "# העלאה\n",
    "lengths_dst = f'gs://{bucket_name}/{INDEX_DIR}_idx/{lengths_filename}'\n",
    "!gsutil cp $lengths_filename $lengths_dst\n",
    "\n",
    "print(f\"Document lengths saved to {lengths_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## סיכום\n",
    "\n",
    "יצרנו:\n",
    "1. **`strong_phrases.pkl`** - סט של ביטויים חזקים לפי PMI\n",
    "2. **`index.pkl`** - אינדקס הפוך עם תמיכה בביטויים\n",
    "3. **`doc_lengths.pickle`** - אורכי מסמכים\n",
    "\n",
    "### איך להשתמש ב-Frontend?\n",
    "\n",
    "ב-`search_frontend.py` צריך:\n",
    "```python\n",
    "# טעינת הביטויים\n",
    "with open('strong_phrases.pkl', 'rb') as f:\n",
    "    strong_phrases = pickle.load(f)\n",
    "\n",
    "# עיבוד השאילתה באותו אופן\n",
    "def process_query(query):\n",
    "    return tokenize_with_phrases(query, strong_phrases, all_stopwords)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning",
   "metadata": {},
   "source": [
    "## טיפים לכיול\n",
    "\n",
    "| פרמטר | ערך נמוך | ערך גבוה |\n",
    "|--------|----------|----------|\n",
    "| `PMI_THRESHOLD` | הרבה ביטויים (כולל רעש) | רק ביטויים \"ברורים\" מאוד |\n",
    "| `MIN_PHRASE_FREQ` | ביטויים נדירים נכללים | רק ביטויים נפוצים |\n",
    "| `MIN_BIGRAM_FREQ` | יותר זיכרון, יותר מועמדים | פחות זיכרון, פחות מועמדים |\n",
    "\n",
    "### ערכים מומלצים:\n",
    "- **לכותרות (titles)**: PMI_THRESHOLD=4, MIN_PHRASE_FREQ=5\n",
    "- **לגוף הטקסט (text)**: PMI_THRESHOLD=6, MIN_PHRASE_FREQ=20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
