{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Text Inverted Index with Pre-computed Phrases\n",
    "\n",
    "**Important:** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!\n",
    "\n",
    "This notebook loads pre-computed phrases from `strong_phrases.pkl` instead of calculating PMI from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check graphframes jar\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# IMPORTANT: Change bucket_name to your bucket!\n",
    "# ==================================\n",
    "bucket_name = 'db204905756'  # <-- ×©× ×” ×œ×©× ×”-bucket ×©×œ×š\n",
    "\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords & Stemmer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Porter Stemmer\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\", \"make\", \"made\",\n",
    "                    \"new\", \"list\", \"district\", \"com\", \"began\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\"-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")\n",
    "print(f\"Stemmer initialized: {type(STEMMER).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data - Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Select BODY TEXT (not title!)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "\n",
    "print(f\"Total documents: {parquetFile.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample\n",
    "sample = doc_text_pairs.take(2)\n",
    "print(f\"Sample doc ID: {sample[0][1]}\")\n",
    "print(f\"Sample text (first 500 chars): {sample[0][0][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”´ Load Pre-computed Phrases from GCS\n",
    "\n",
    "**Instead of calculating PMI, we load the already computed phrases!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download strong_phrases.pkl from GCS\n",
    "# Try different possible locations\n",
    "\n",
    "possible_paths = [\n",
    "    f'gs://{bucket_name}/phrases/strong_phrases.pkl',\n",
    "    f'gs://{bucket_name}/strong_phrases.pkl',\n",
    "    f'gs://{bucket_name}/body_stemmed_phrases_idx/strong_phrases_body_stemmed.pkl',\n",
    "    f'gs://{bucket_name}/title_with_phrases_idx/strong_phrases.pkl'\n",
    "]\n",
    "\n",
    "print(\"Searching for strong_phrases.pkl in GCS...\")\n",
    "for gcs_path in possible_paths:\n",
    "    result = !gsutil ls {gcs_path} 2>/dev/null\n",
    "    if result and 'CommandException' not in str(result):\n",
    "        print(f\"âœ… Found: {gcs_path}\")\n",
    "        !gsutil cp {gcs_path} strong_phrases.pkl\n",
    "        break\n",
    "else:\n",
    "    print(\"\\nâŒ Could not find strong_phrases.pkl. Let's search the bucket:\")\n",
    "    !gsutil ls -r gs://$bucket_name/ | grep -i phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the phrases\n",
    "with open('strong_phrases.pkl', 'rb') as f:\n",
    "    strong_phrases = pickle.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded {len(strong_phrases):,} pre-computed phrases!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample phrases:\")\n",
    "for i, phrase in enumerate(list(strong_phrases)[:20]):\n",
    "    print(f\"  {phrase[0]}_{phrase[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Functions (with Stemming + Phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_phrases_stemmed(text, phrases_set, stopwords_set):\n",
    "    \"\"\"\n",
    "    Tokenize text with stemming, replacing recognized phrases with single tokens.\n",
    "    \n",
    "    Example:\n",
    "    \"I live in new york city\" -> [\"live\", \"new_york\", \"citi\"]  (stemmed!)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    # Extract and stem tokens\n",
    "    raw_tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    tokens = [STEMMER.stem(t) for t in raw_tokens if t not in stopwords_set]\n",
    "    \n",
    "    if len(tokens) <= 1:\n",
    "        return tokens\n",
    "    \n",
    "    # Merge phrases\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            if bigram in phrases_set:\n",
    "                result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                i += 2\n",
    "                continue\n",
    "        result.append(tokens[i])\n",
    "        i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"The United States and New York City are located in North America\"\n",
    "test_result = tokenize_with_phrases_stemmed(test_text, strong_phrases, all_stopwords)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Output (stemmed + phrases): {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load InvertedIndex Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InvertedIndex module\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast phrases and stopwords to all workers\n",
    "strong_phrases_broadcast = sc.broadcast(strong_phrases)\n",
    "all_stopwords_broadcast = sc.broadcast(all_stopwords)\n",
    "\n",
    "print(f\"âœ… Broadcasted {len(strong_phrases):,} phrases to all workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Inverted Index Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "INDEX_DIR = \"body_stemmed_phrases\"  # Directory name for body index with stemming\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"Map token to bucket number\"\"\"\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count_with_phrases_stemmed(text, doc_id):\n",
    "    \"\"\"\n",
    "    Count term frequency for each stemmed token (including phrases) in document.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_with_phrases_stemmed(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    counts = Counter(tokens)\n",
    "    return [(token, (doc_id, tf)) for token, tf in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Sort posting list by doc_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each token\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    \"\"\"Partition and write posting lists to GCS\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return InvertedIndex.write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), base_dir, bucket_name\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Building inverted index with stemming and phrases for body text...\")\n",
    "print(\"This will take a while for the full corpus...\")\n",
    "\n",
    "# Step 1: Word counts (stemmed + phrases)\n",
    "word_counts = doc_text_pairs.flatMap(lambda x: word_count_with_phrases_stemmed(x[0], x[1]))\n",
    "\n",
    "# Step 2: Create posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Step 3: Filter rare terms (helps reduce index size)\n",
    "MIN_DF = 50  # Minimum document frequency\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) >= MIN_DF)\n",
    "\n",
    "# Step 4: Calculate df\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"Total unique tokens (with df >= {MIN_DF}): {len(w2df_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Write posting lists to GCS\n",
    "print(f\"Writing posting lists to {INDEX_DIR}...\")\n",
    "_ = partition_postings_and_write(postings_filtered, INDEX_DIR).collect()\n",
    "print(\"Done writing posting lists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all posting list locations\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix=INDEX_DIR):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "print(f\"Collected posting locations for {len(super_posting_locs):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save InvertedIndex\n",
    "inverted = InvertedIndex()\n",
    "inverted.posting_locs = super_posting_locs\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# Save locally\n",
    "inverted.write_index('.', 'index')\n",
    "\n",
    "# Upload to GCS\n",
    "index_src = \"index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/body_stemmed_phrases_idx/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "\n",
    "print(f\"Index saved to {index_dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index size\n",
    "!gsutil ls -lh $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doc_length_stemmed(doc_id, text):\n",
    "    \"\"\"Calculate document length in stemmed tokens (including phrases)\"\"\"\n",
    "    tokens = tokenize_with_phrases_stemmed(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    return (doc_id, len(tokens))\n",
    "\n",
    "# Calculate lengths\n",
    "print(\"Calculating document lengths (stemmed)...\")\n",
    "doc_lengths_rdd = doc_text_pairs.map(lambda x: calc_doc_length_stemmed(x[1], x[0]))\n",
    "doc_lengths_dict = doc_lengths_rdd.collectAsMap()\n",
    "\n",
    "print(f\"Calculated lengths for {len(doc_lengths_dict):,} documents\")\n",
    "\n",
    "# Stats\n",
    "lengths = list(doc_lengths_dict.values())\n",
    "print(f\"Average doc length: {np.mean(lengths):.1f}\")\n",
    "print(f\"Median doc length: {np.median(lengths):.1f}\")\n",
    "print(f\"Max doc length: {max(lengths):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document lengths\n",
    "lengths_filename = 'body_doc_lengths_stemmed.pickle'\n",
    "\n",
    "with open(lengths_filename, 'wb') as f:\n",
    "    pickle.dump(doc_lengths_dict, f)\n",
    "\n",
    "# Upload to GCS\n",
    "lengths_dst = f'gs://{bucket_name}/body_stemmed_phrases_idx/{lengths_filename}'\n",
    "!gsutil cp $lengths_filename $lengths_dst\n",
    "\n",
    "print(f\"Document lengths saved to {lengths_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also save the phrases to the index folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy phrases file to the index folder for convenience\n",
    "phrases_dst = f'gs://{bucket_name}/body_stemmed_phrases_idx/strong_phrases.pkl'\n",
    "!gsutil cp strong_phrases.pkl $phrases_dst\n",
    "print(f\"Phrases also saved to {phrases_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files in body_stemmed_phrases_idx/:\")\n",
    "!gsutil ls -lh gs://$bucket_name/body_stemmed_phrases_idx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPosting list files in body_stemmed_phrases/:\")\n",
    "!gsutil ls gs://$bucket_name/body_stemmed_phrases/ | head -20\n",
    "print(\"...\")\n",
    "!gsutil ls gs://$bucket_name/body_stemmed_phrases/ | wc -l\n",
    "print(\"total files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Created:\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `index.pkl` | `body_stemmed_phrases_idx/` | Inverted index (posting locs + df) |\n",
    "| `body_doc_lengths_stemmed.pickle` | `body_stemmed_phrases_idx/` | Document lengths for BM25 |\n",
    "| `strong_phrases.pkl` | `body_stemmed_phrases_idx/` | Set of detected phrases |\n",
    "| `*.bin` | `body_stemmed_phrases/` | Binary posting list files |\n",
    "\n",
    "### Usage in search_frontend.py:\n",
    "\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# Load phrases\n",
    "with open('strong_phrases.pkl', 'rb') as f:\n",
    "    strong_phrases = pickle.load(f)\n",
    "\n",
    "# Process query with same tokenization (stemming + phrases)\n",
    "def process_query(query):\n",
    "    return tokenize_with_phrases_stemmed(query, strong_phrases, all_stopwords)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… Body text index with pre-computed phrases creation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
