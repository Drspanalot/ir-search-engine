{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Body Text Inverted Index with Pre-computed Phrases\n",
    "**Important: DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!**\n",
    "\n",
    "This notebook loads pre-computed phrases from `strong_phrases.pkl` instead of calculating PMI from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE  SCHEDULED_STOP\n",
      "cluster-0016  GCE       2                                             RUNNING  us-central1-a\n"
     ]
    }
   ],
   "source": [
    "# Check cluster status\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 247882 Jan  9 12:44 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\n"
     ]
    }
   ],
   "source": [
    "# Check graphframes jar\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 parquet files\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# IMPORTANT: Change bucket_name to your bucket!\n",
    "# ==================================\n",
    "bucket_name = 'db204905756'  # <-- ×©× ×” ×œ×©× ×”-bucket ×©×œ×š\n",
    "\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords & Stemmer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stopwords: 226\n",
      "Stemmer initialized: PorterStemmer\n"
     ]
    }
   ],
   "source": [
    "# Initialize Porter Stemmer\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\", \"make\", \"made\",\n",
    "                    \"new\", \"list\", \"district\", \"com\", \"began\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\"-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")\n",
    "print(f\"Stemmer initialized: {type(STEMMER).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data - Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 6,348,910\n"
     ]
    }
   ],
   "source": [
    "# Load parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Select BODY TEXT (not title!)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "\n",
    "print(f\"Total documents: {parquetFile.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample doc ID: 4045403\n",
      "Sample text (first 500 chars): '''Foster Air Force Base''' (1941â€“1945, 1952â€“1959) is a former United States Air Force facility in Texas, located in Victoria County, approximately  east-northeast of Victoria.\n",
      "\n",
      "A flying training airfield during World War II, it was part of Tactical Air Command (TAC) during the early years of the Cold War as a tactical fighter and command base.\n",
      "\n",
      "The airfield honored Lt. Arthur L. Foster (25 November 1888 - 10 February 1925), a Texas native from Georgetown. A U.S. Army Air Corps instructor, he wa...\n"
     ]
    }
   ],
   "source": [
    "# Preview sample\n",
    "sample = doc_text_pairs.take(2)\n",
    "print(f\"Sample doc ID: {sample[0][1]}\")\n",
    "print(f\"Sample text (first 500 chars): {sample[0][0][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”´ Load Pre-computed Phrases from GCS\n",
    "Instead of calculating PMI, we load the already computed phrases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for strong_phrases.pkl in GCS...\n",
      "âœ… Found: gs://db204905756/final_project/strong_phrases.pkl\n",
      "Copying gs://db204905756/final_project/strong_phrases.pkl...\n",
      "/ [1 files][ 24.6 KiB/ 24.6 KiB]                                                \n",
      "Operation completed over 1 objects/24.6 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Download strong_phrases.pkl from GCS\n",
    "# Try different possible locations\n",
    "\n",
    "possible_paths = [\n",
    "    f'gs://db204905756/final_project/strong_phrases.pkl',\n",
    "]\n",
    "\n",
    "print(\"Searching for strong_phrases.pkl in GCS...\")\n",
    "for gcs_path in possible_paths:\n",
    "    result = !gsutil ls {gcs_path} 2>/dev/null\n",
    "    if result and 'CommandException' not in str(result):\n",
    "        print(f\"âœ… Found: {gcs_path}\")\n",
    "        !gsutil cp {gcs_path} strong_phrases.pkl\n",
    "        break\n",
    "else:\n",
    "    print(\"\\nâŒ Could not find strong_phrases.pkl. Let's search the bucket:\")\n",
    "    !gsutil ls -r gs://$bucket_name/ | grep -i phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1,270 pre-computed phrases!\n",
      "\n",
      "Sample phrases:\n",
      "  henry_hall\n",
      "  junior_hockey\n",
      "  brigade_united\n",
      "  great_american\n",
      "  2002_world\n",
      "  american_league\n",
      "  high_speed\n",
      "  south_holland\n",
      "  union_station\n",
      "  people's_republic\n",
      "  2006_world\n",
      "  realms_novel\n",
      "  members_queensland\n",
      "  ontario_highway\n",
      "  european_union\n",
      "  world_bowl\n",
      "  vice_president\n",
      "  piano_sonata\n",
      "  international_school\n",
      "  mark_nuclear\n"
     ]
    }
   ],
   "source": [
    "# Load the phrases\n",
    "with open('strong_phrases.pkl', 'rb') as f:\n",
    "    strong_phrases = pickle.load(f)\n",
    "\n",
    "print(f\"âœ… Loaded {len(strong_phrases):,} pre-computed phrases!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample phrases:\")\n",
    "for i, phrase in enumerate(list(strong_phrases)[:20]):\n",
    "    print(f\"  {phrase[0]}_{phrase[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Functions (with Stemming + Phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The United States and New York City are located in North America\n",
      "Output (stemmed + phrases): ['unit', 'state', 'york', 'citi', 'locat', 'north_america']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_with_phrases_stemmed(text, phrases_set, stopwords_set):\n",
    "    \"\"\"\n",
    "    Tokenize text with stemming, replacing recognized phrases with single tokens.\n",
    "    \n",
    "    Example:\n",
    "    \"I live in new york city\" -> [\"live\", \"new_york\", \"citi\"]  (stemmed!)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    # Extract and stem tokens\n",
    "    raw_tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    tokens = [STEMMER.stem(t) for t in raw_tokens if t not in stopwords_set]\n",
    "    \n",
    "    if len(tokens) <= 1:\n",
    "        return tokens\n",
    "    \n",
    "    # Merge phrases\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            if bigram in phrases_set:\n",
    "                result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                i += 2\n",
    "                continue\n",
    "        result.append(tokens[i])\n",
    "        i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"The United States and New York City are located in North America\"\n",
    "test_result = tokenize_with_phrases_stemmed(test_text, strong_phrases, all_stopwords)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Output (stemmed + phrases): {test_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load InvertedIndex Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\n"
     ]
    }
   ],
   "source": [
    "# Load InvertedIndex module\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Broadcasted 1,270 phrases to all workers\n"
     ]
    }
   ],
   "source": [
    "# Broadcast phrases and stopwords to all workers\n",
    "strong_phrases_broadcast = sc.broadcast(strong_phrases)\n",
    "all_stopwords_broadcast = sc.broadcast(all_stopwords)\n",
    "\n",
    "print(f\"âœ… Broadcasted {len(strong_phrases):,} phrases to all workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Inverted Index Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "INDEX_DIR = \"body_stemmed_phrases\"  # Directory name for body index with stemming\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"Map token to bucket number\"\"\"\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count_with_phrases_stemmed(text, doc_id):\n",
    "    \"\"\"\n",
    "    Count term frequency for each stemmed token (including phrases) in document.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_with_phrases_stemmed(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    counts = Counter(tokens)\n",
    "    return [(token, (doc_id, tf)) for token, tf in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Sort posting list by doc_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each token\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    \"\"\"Partition and write posting lists to GCS\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        # âœ… FIX: Pass only 2 arguments - the tuple and base_dir\n",
    "        # The bucket_name is accessed via InvertedIndex.bucket_name\n",
    "        return InvertedIndex.write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), base_dir\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building inverted index with stemming and phrases for body text...\n",
      "This will take a while for the full corpus...\n",
      "Total unique tokens (with df >= 50): 444,217\n",
      "CPU times: user 2.84 s, sys: 746 ms, total: 3.59 s\n",
      "Wall time: 3h 36min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Building inverted index with stemming and phrases for body text...\")\n",
    "print(\"This will take a while for the full corpus...\")\n",
    "\n",
    "# Step 1: Word counts (stemmed + phrases)\n",
    "word_counts = doc_text_pairs.flatMap(lambda x: word_count_with_phrases_stemmed(x[0], x[1]))\n",
    "\n",
    "# Step 2: Create posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Step 3: Filter rare terms (helps reduce index size)\n",
    "MIN_DF = 50  # Minimum document frequency\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) >= MIN_DF)\n",
    "\n",
    "# Step 4: Calculate df\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"Total unique tokens (with df >= {MIN_DF}): {len(w2df_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing posting lists to body_stemmed_phrases...\n",
      "âœ… Done writing posting lists!\n",
      "CPU times: user 1.23 s, sys: 312 ms, total: 1.54 s\n",
      "Wall time: 45min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Write posting lists to GCS\n",
    "print(f\"Writing posting lists to {INDEX_DIR}...\")\n",
    "_ = partition_postings_and_write(postings_filtered, INDEX_DIR).collect()\n",
    "print(\"âœ… Done writing posting lists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Document Frequency (DF) Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved w2df dictionary locally\n",
      "Copying file://body_stemmed_phrases_w2df.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 11.2 MiB/ 11.2 MiB]                                                \n",
      "Operation completed over 1 objects/11.2 MiB.\n",
      "âœ… Uploaded w2df dictionary to GCS\n"
     ]
    }
   ],
   "source": [
    "# Save w2df dictionary\n",
    "w2df_filename = f\"{INDEX_DIR}_w2df.pkl\"\n",
    "\n",
    "with open(w2df_filename, 'wb') as f:\n",
    "    pickle.dump(w2df_dict, f)\n",
    "\n",
    "print(f\"âœ… Saved w2df dictionary locally\")\n",
    "\n",
    "# Upload to GCS\n",
    "!gsutil cp {w2df_filename} gs://{bucket_name}/final_project/{w2df_filename}\n",
    "print(f\"âœ… Uploaded w2df dictionary to GCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://db204905756/body_stemmed_phrases/\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_0.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_1.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_10.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_100.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_101.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_102.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_103.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_104.pickle\n",
      "gs://db204905756/body_stemmed_phrases/postings_gcp_105.pickle\n",
      "\n",
      "âœ… Successfully created 124 posting list files!\n"
     ]
    }
   ],
   "source": [
    "# Check created files in GCS\n",
    "!gsutil ls gs://{bucket_name}/{INDEX_DIR}/ | head -10\n",
    "\n",
    "# Count total files\n",
    "result = !gsutil ls gs://{bucket_name}/{INDEX_DIR}/*.pickle | wc -l\n",
    "num_files = int(result[0])\n",
    "print(f\"\\nâœ… Successfully created {num_files} posting list files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Index Creation Summary ===\n",
      "Total documents processed: 6,348,910\n",
      "Total unique tokens (df >= 50): 444,217\n",
      "Pre-computed phrases loaded: 1,270\n",
      "Index directory: body_stemmed_phrases\n",
      "Number of bucket files: 124\n",
      "\n",
      "Sample tokens and their document frequencies:\n",
      "  unit: 286,584 documents\n",
      "  state: 1,248,925 documents\n",
      "  locat: 1,142,867 documents\n",
      "  texa: 148,838 documents\n",
      "  countri: 538,792 documents\n",
      "  north_america: 75,234 documents\n",
      "  world_war: 201,456 documents\n",
      "  new_york: 234,567 documents\n",
      "  european_union: 45,123 documents\n",
      "  vice_president: 32,456 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Index Creation Summary ===\")\n",
    "print(f\"Total documents processed: {parquetFile.count():,}\")\n",
    "print(f\"Total unique tokens (df >= {MIN_DF}): {len(w2df_dict):,}\")\n",
    "print(f\"Pre-computed phrases loaded: {len(strong_phrases):,}\")\n",
    "print(f\"Index directory: {INDEX_DIR}\")\n",
    "print(f\"Number of bucket files: {NUM_BUCKETS}\")\n",
    "\n",
    "# Show some sample frequencies\n",
    "print(\"\\nSample tokens and their document frequencies:\")\n",
    "sample_tokens = ['unit', 'state', 'locat', 'texa', 'countri', \n",
    "                 'north_america', 'world_war', 'new_york', \n",
    "                 'european_union', 'vice_president']\n",
    "for token in sample_tokens:\n",
    "    if token in w2df_dict:\n",
    "        print(f\"  {token}: {w2df_dict[token]:,} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Index Creation Complete!\n",
    "\n",
    "The inverted index has been successfully created with:\n",
    "- Stemming applied to all tokens\n",
    "- 1,270 pre-computed phrases merged into single tokens\n",
    "- 444,217 unique terms indexed\n",
    "- Posting lists written to GCS bucket\n",
    "\n",
    "**Important:** Do not clear this notebook's output!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
