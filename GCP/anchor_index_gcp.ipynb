{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor Text Inverted Index\n",
    "\n",
    "**Important: DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!**\n",
    "\n",
    "**Output folder:** `anchor_postings_gcp/` (will NOT overwrite body index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# BUCKET NAME\n",
    "# ==================================\n",
    "bucket_name = 'db204905756'\n",
    "\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd\n",
    "print(f\"Total documents: {parquetFile.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration & Custom Writer\n",
    "\n",
    "**IMPORTANT:** We use a custom folder `anchor_postings_gcp/` to avoid overwriting the body index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# CONFIGURATION\n",
    "# ==================================\n",
    "POSTINGS_FOLDER = \"anchor_postings_gcp\"  # Custom folder for anchor index!\n",
    "BLOCK_SIZE = 1999998\n",
    "TUPLE_SIZE = 6\n",
    "TF_MASK = 2 ** 16 - 1\n",
    "\n",
    "print(f\"✅ Posting lists will be written to: gs://{bucket_name}/{POSTINGS_FOLDER}/\")\n",
    "print(f\"   This will NOT overwrite your body index in postings_gcp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords and setup\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = ['category', 'references', 'also', 'links', 'external',\n",
    "                    'first', 'see', 'new', 'two', 'list', 'may', 'one', 'district',\n",
    "                    'including', 'became', 'however', 'com', 'many', 'began',\n",
    "                    'make', 'made', 'part', 'would', 'people', 'second',\n",
    "                    'following', 'history', 'thumb']\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\"-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "STEMMER = PorterStemmer()\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# CUSTOM MULTIFILEWRITER - writes to anchor_postings_gcp/\n",
    "# ==================================\n",
    "\n",
    "class MultiFileWriter:\n",
    "    \"\"\"Sequential binary writer to multiple files - writes to custom folder\"\"\"\n",
    "    def __init__(self, base_dir, name, bucket_name, folder_name):\n",
    "        self._base_dir = Path(base_dir)\n",
    "        self._name = name\n",
    "        self._folder_name = folder_name\n",
    "        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') \n",
    "                          for i in itertools.count())\n",
    "        self._f = next(self._file_gen)\n",
    "        self.client = storage.Client()\n",
    "        self.bucket = self.client.bucket(bucket_name)\n",
    "        \n",
    "    def write(self, b):\n",
    "        locs = []\n",
    "        while len(b) > 0:\n",
    "            pos = self._f.tell()\n",
    "            remaining = BLOCK_SIZE - pos\n",
    "            if remaining == 0:  \n",
    "                self._f.close()\n",
    "                self.upload_to_gcp()                \n",
    "                self._f = next(self._file_gen)\n",
    "                pos, remaining = 0, BLOCK_SIZE\n",
    "            self._f.write(b[:remaining])\n",
    "            locs.append((self._f.name, pos))\n",
    "            b = b[remaining:]\n",
    "        return locs\n",
    "\n",
    "    def close(self):\n",
    "        self._f.close()\n",
    "    \n",
    "    def upload_to_gcp(self):\n",
    "        file_name = self._f.name\n",
    "        # Use custom folder name!\n",
    "        blob = self.bucket.blob(f\"{self._folder_name}/{file_name}\")\n",
    "        blob.upload_from_filename(file_name)\n",
    "\n",
    "\n",
    "def write_a_posting_list(b_w_pl, bucket_name, folder_name):\n",
    "    \"\"\"Write posting list to GCS with custom folder\"\"\"\n",
    "    posting_locs = defaultdict(list)\n",
    "    bucket_id, list_w_pl = b_w_pl\n",
    "    \n",
    "    with closing(MultiFileWriter(\".\", bucket_id, bucket_name, folder_name)) as writer:\n",
    "        for w, pl in list_w_pl: \n",
    "            b = b''.join([(doc_id << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')\n",
    "                          for doc_id, tf in pl])\n",
    "            locs = writer.write(b)\n",
    "            posting_locs[w].extend(locs)\n",
    "        writer.upload_to_gcp()\n",
    "        \n",
    "        # Upload posting locations pickle to custom folder\n",
    "        with open(f\"{bucket_id}_posting_locs.pickle\", \"wb\") as f:\n",
    "            pickle.dump(dict(posting_locs), f)\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(f\"{folder_name}/{bucket_id}_posting_locs.pickle\")\n",
    "        blob.upload_from_filename(f\"{bucket_id}_posting_locs.pickle\")\n",
    "        \n",
    "    return bucket_id\n",
    "\n",
    "print(\"✅ Custom MultiFileWriter defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Building Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_postings_and_write(postings):\n",
    "    \"\"\"Partition postings by bucket and write to GCS - uses anchor_postings_gcp folder\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), \n",
    "            bucket_name, \n",
    "            POSTINGS_FOLDER  # <-- Custom folder!\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Returns a sorted posting list by wiki_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each token\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize and stem text\"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    stemmed = [STEMMER.stem(token) for token in tokens if token not in all_stopwords]\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def stemmed_word_count(text, doc_id):\n",
    "    \"\"\"Count stemmed words in text for a given doc_id\"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    token_counts = Counter(tokens)\n",
    "    return [(token, (doc_id, count)) for token, count in token_counts.items()]\n",
    "\n",
    "print(\"✅ Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Anchor Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten anchor text pairs\n",
    "# Each document can have multiple anchor texts pointing to it\n",
    "new_pairs = doc_anchor_pairs.flatMap(\n",
    "    lambda pair: [(p.text, p.id) for p in pair[0]] if pair[0] else []\n",
    ")\n",
    "\n",
    "print(\"Sample anchor texts:\")\n",
    "for sample in new_pairs.take(5):\n",
    "    print(f\"  Text: '{sample[0]}' -> Doc ID: {sample[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Building anchor text index...\")\n",
    "\n",
    "# Word counts\n",
    "word_counts_anchor = new_pairs.flatMap(lambda x: stemmed_word_count(x[0], x[1]))\n",
    "\n",
    "# Group and reduce\n",
    "posting_anchor = word_counts_anchor.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Count unique doc occurrences (a doc might be linked multiple times with same anchor)\n",
    "def new_count(posting):\n",
    "    counts = Counter(p[0] for p in posting)\n",
    "    return list(counts.items())\n",
    "\n",
    "postings = posting_anchor.map(lambda x: (x[0], new_count(x[1])))\n",
    "\n",
    "# Cache to avoid recomputation\n",
    "postings.cache()\n",
    "print(f\"✅ Postings cached\")\n",
    "print(f\"Sample: {postings.take(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate df and collect\n",
    "print(\"Calculating document frequencies...\")\n",
    "w2df = calculate_df(postings)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "print(f\"✅ Vocabulary size: {len(w2df_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Write posting lists to GCS\n",
    "print(f\"Writing posting lists to {POSTINGS_FOLDER}/...\")\n",
    "_ = partition_postings_and_write(postings).collect()\n",
    "print(\"✅ Done writing posting lists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Posting Locations & Save Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect posting locations from the ANCHOR folder (not postings_gcp!)\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix=POSTINGS_FOLDER):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "print(f\"✅ Collected {len(super_posting_locs):,} posting locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create InvertedIndex class for saving\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.df = {}\n",
    "        self.posting_locs = {}\n",
    "    \n",
    "    def write_index(self, base_dir, name):\n",
    "        with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "# Create and save index\n",
    "inverted = InvertedIndex()\n",
    "inverted.posting_locs = dict(super_posting_locs)\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# Save locally\n",
    "inverted.write_index('.', 'anchor_index')\n",
    "\n",
    "# Upload to GCS\n",
    "index_src = \"anchor_index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "\n",
    "print(f\"\\n✅ Index saved to {index_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"Files in anchor_index/:\")\n",
    "print(\"=\" * 50)\n",
    "!gsutil ls -lh gs://$bucket_name/anchor_index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nPosting list files in {POSTINGS_FOLDER}/:\")\n",
    "!gsutil ls gs://$bucket_name/anchor_postings_gcp/ | head -20\n",
    "print(\"...\")\n",
    "!gsutil ls gs://$bucket_name/anchor_postings_gcp/ | wc -l\n",
    "print(\"total files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify body index is UNTOUCHED\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Verifying body index is still intact:\")\n",
    "print(\"=\" * 50)\n",
    "!gsutil ls gs://$bucket_name/postings_gcp/*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Files Created:\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| anchor_index.pkl | anchor_index/ | Inverted index (posting locs + df) |\n",
    "| *.bin | anchor_postings_gcp/ | Binary posting list files |\n",
    "| *_posting_locs.pickle | anchor_postings_gcp/ | Posting location files |\n",
    "\n",
    "### Your other indexes are SAFE:\n",
    "- `postings_gcp/` - Body index ✅\n",
    "- `title_postingsPhrases_gcp/` - Title index ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ Anchor Text Index - COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nVocabulary size: {len(w2df_dict):,}\")\n",
    "print(f\"\\nIndex location: gs://{bucket_name}/anchor_index/\")\n",
    "print(f\"Posting lists: gs://{bucket_name}/{POSTINGS_FOLDER}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
