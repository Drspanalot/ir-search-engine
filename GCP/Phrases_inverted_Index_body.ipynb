{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrases Inverted Index - Body Text (with Stemming)\n",
    "\n",
    "**מטרה:** בניית Inverted Index על גוף הטקסט (body) עם תמיכה בביטויים (phrases) באמצעות PMI + Porter Stemming\n",
    "\n",
    "***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "!gcloud dataproc clusters list --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from contextlib import closing\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check graphframes jar\n",
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# IMPORTANT: Change bucket_name to your bucket!\n",
    "# ==================================\n",
    "bucket_name = 'db204905756'  # <-- שנה לשם ה-bucket שלך\n",
    "\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths = []\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if \"parquet\" in b.name:\n",
    "        paths.append(full_path + b.name)\n",
    "\n",
    "print(f\"Found {len(paths)} parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords, Regex & Stemmer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Porter Stemmer\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\", \"make\", \"made\",\n",
    "                    \"new\", \"list\", \"district\", \"com\", \"began\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "print(f\"Total stopwords: {len(all_stopwords)}\")\n",
    "print(f\"Stemmer initialized: {type(STEMMER).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data - Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parquet files\n",
    "parquetFile = spark.read.parquet(*paths)\n",
    "\n",
    "# Select BODY TEXT (not title!)\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "\n",
    "print(f\"Total documents: {parquetFile.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample\n",
    "sample = doc_text_pairs.take(2)\n",
    "print(f\"Sample doc ID: {sample[0][1]}\")\n",
    "print(f\"Sample text (first 500 chars): {sample[0][0][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI-Based Phrase Detection (with Stemming)\n",
    "\n",
    "## מה זה PMI?\n",
    "PMI (Pointwise Mutual Information) מודד את הסיכוי ששתי מילים מופיעות יחד בהשוואה לסיכוי שהן מופיעות בנפרד:\n",
    "\n",
    "$$PMI(word_1, word_2) = \\log_2 \\frac{P(word_1, word_2)}{P(word_1) \\cdot P(word_2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unigrams_stemmed(text):\n",
    "    \"\"\"\n",
    "    Extract all stemmed words (unigrams) from text\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    # Remove stopwords and apply stemming\n",
    "    stemmed = [STEMMER.stem(t) for t in tokens if t not in all_stopwords]\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def extract_bigrams_stemmed(text):\n",
    "    \"\"\"\n",
    "    Extract all adjacent stemmed word pairs (bigrams) from text\n",
    "    \"\"\"\n",
    "    tokens = extract_unigrams_stemmed(text)\n",
    "    bigrams = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigrams.append((tokens[i], tokens[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def calculate_pmi(unigram_counts, bigram_counts, total_unigrams, total_bigrams):\n",
    "    \"\"\"\n",
    "    Calculate PMI for all bigrams\n",
    "    \n",
    "    PMI(w1, w2) = log2( P(w1, w2) / (P(w1) * P(w2)) )\n",
    "    \"\"\"\n",
    "    pmi_scores = {}\n",
    "    \n",
    "    for (w1, w2), bigram_count in bigram_counts.items():\n",
    "        p_bigram = bigram_count / total_bigrams\n",
    "        p_w1 = unigram_counts.get(w1, 0) / total_unigrams\n",
    "        p_w2 = unigram_counts.get(w2, 0) / total_unigrams\n",
    "        \n",
    "        if p_w1 > 0 and p_w2 > 0:\n",
    "            pmi = math.log2(p_bigram / (p_w1 * p_w2))\n",
    "            pmi_scores[(w1, w2)] = pmi\n",
    "    \n",
    "    return pmi_scores\n",
    "\n",
    "\n",
    "# Test stemming\n",
    "test_words = [\"running\", \"cities\", \"better\", \"university\", \"played\"]\n",
    "print(\"Stemming examples:\")\n",
    "for word in test_words:\n",
    "    print(f\"  {word} -> {STEMMER.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Count Unigrams (Stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Counting stemmed unigrams (this may take a while for body text)...\")\n",
    "\n",
    "unigram_rdd = doc_text_pairs.flatMap(lambda x: extract_unigrams_stemmed(x[0]))\n",
    "unigram_counts_rdd = unigram_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect to memory\n",
    "unigram_counts = dict(unigram_counts_rdd.collect())\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "print(f\"Unique stemmed unigrams: {len(unigram_counts):,}\")\n",
    "print(f\"Total unigram occurrences: {total_unigrams:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Count Bigrams (Stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For body text, we need higher minimum frequency to filter noise\n",
    "MIN_BIGRAM_FREQ = 100  # Higher threshold for body text\n",
    "\n",
    "print(f\"Counting stemmed bigrams (min frequency: {MIN_BIGRAM_FREQ})...\")\n",
    "\n",
    "bigram_rdd = doc_text_pairs.flatMap(lambda x: extract_bigrams_stemmed(x[0]))\n",
    "bigram_counts_rdd = bigram_rdd.map(lambda bg: (bg, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Filter by minimum frequency\n",
    "bigram_counts_filtered = bigram_counts_rdd.filter(lambda x: x[1] >= MIN_BIGRAM_FREQ)\n",
    "\n",
    "# Collect to memory\n",
    "bigram_counts = dict(bigram_counts_filtered.collect())\n",
    "total_bigrams = sum(bigram_counts.values())\n",
    "\n",
    "print(f\"Bigrams with freq >= {MIN_BIGRAM_FREQ}: {len(bigram_counts):,}\")\n",
    "print(f\"Total bigram occurrences: {total_bigrams:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate PMI and Filter Strong Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# PMI thresholds for body text\n",
    "PMI_THRESHOLD = 6.0  # Higher threshold for body text\n",
    "MIN_PHRASE_FREQ = 50  # Minimum frequency for a phrase\n",
    "\n",
    "print(f\"Calculating PMI (threshold: {PMI_THRESHOLD}, min_freq: {MIN_PHRASE_FREQ})...\")\n",
    "\n",
    "# Calculate PMI\n",
    "pmi_scores = calculate_pmi(unigram_counts, bigram_counts, total_unigrams, total_bigrams)\n",
    "\n",
    "# Filter strong phrases\n",
    "strong_phrases = set()\n",
    "phrase_stats = []\n",
    "\n",
    "for (w1, w2), pmi in pmi_scores.items():\n",
    "    freq = bigram_counts.get((w1, w2), 0)\n",
    "    if pmi >= PMI_THRESHOLD and freq >= MIN_PHRASE_FREQ:\n",
    "        strong_phrases.add((w1, w2))\n",
    "        phrase_stats.append(((w1, w2), pmi, freq))\n",
    "\n",
    "print(f\"Strong phrases found: {len(strong_phrases):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top phrases by PMI\n",
    "phrase_stats_sorted = sorted(phrase_stats, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 30 stemmed phrases by PMI:\")\n",
    "print(\"-\" * 60)\n",
    "for (w1, w2), pmi, freq in phrase_stats_sorted[:30]:\n",
    "    print(f\"{w1}_{w2:20s} PMI: {pmi:.2f}  Freq: {freq:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top phrases by frequency\n",
    "phrase_stats_by_freq = sorted(phrase_stats, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\nTop 30 stemmed phrases by Frequency:\")\n",
    "print(\"-\" * 60)\n",
    "for (w1, w2), pmi, freq in phrase_stats_by_freq[:30]:\n",
    "    print(f\"{w1}_{w2:20s} Freq: {freq:,}  PMI: {pmi:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save strong phrases\n",
    "phrases_filename = 'strong_phrases_body_stemmed.pkl'\n",
    "with open(phrases_filename, 'wb') as f:\n",
    "    pickle.dump(strong_phrases, f)\n",
    "\n",
    "# Upload to GCS\n",
    "phrases_dst = f'gs://{bucket_name}/body_stemmed_phrases_idx/{phrases_filename}'\n",
    "!gsutil cp $phrases_filename $phrases_dst\n",
    "\n",
    "print(f\"Saved {len(strong_phrases):,} phrases to {phrases_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Inverted Index with Phrases and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_phrases_stemmed(text, phrases_set, stopwords_set):\n",
    "    \"\"\"\n",
    "    Tokenize text with stemming, replacing recognized phrases with single tokens.\n",
    "    \n",
    "    Example:\n",
    "    \"I live in new york city\" -> [\"live\", \"new_york\", \"citi\"]  (stemmed!)\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return []\n",
    "    \n",
    "    # Extract and stem tokens\n",
    "    raw_tokens = [token.group().lower() for token in RE_WORD.finditer(text)]\n",
    "    tokens = [STEMMER.stem(t) for t in raw_tokens if t not in stopwords_set]\n",
    "    \n",
    "    if len(tokens) <= 1:\n",
    "        return tokens\n",
    "    \n",
    "    # Merge phrases\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1:\n",
    "            bigram = (tokens[i], tokens[i+1])\n",
    "            if bigram in phrases_set:\n",
    "                result.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "                i += 2\n",
    "                continue\n",
    "        result.append(tokens[i])\n",
    "        i += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test\n",
    "test_text = \"The United States and New York City are located in North America\"\n",
    "test_result = tokenize_with_phrases_stemmed(test_text, strong_phrases, all_stopwords)\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Output (stemmed + phrases): {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InvertedIndex module\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())\n",
    "\n",
    "from inverted_index_gcp import InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast phrases and stopwords to all workers\n",
    "strong_phrases_broadcast = sc.broadcast(strong_phrases)\n",
    "all_stopwords_broadcast = sc.broadcast(all_stopwords)\n",
    "\n",
    "print(f\"Broadcasted {len(strong_phrases):,} phrases to all workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = 124\n",
    "INDEX_DIR = \"body_stemmed_phrases\"  # Directory name for body index with stemming\n",
    "\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"Map token to bucket number\"\"\"\n",
    "    return int(_hash(token), 16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def word_count_with_phrases_stemmed(text, doc_id):\n",
    "    \"\"\"\n",
    "    Count term frequency for each stemmed token (including phrases) in document.\n",
    "    \"\"\"\n",
    "    tokens = tokenize_with_phrases_stemmed(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    counts = Counter(tokens)\n",
    "    return [(token, (doc_id, tf)) for token, tf in counts.items()]\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    \"\"\"Sort posting list by doc_id\"\"\"\n",
    "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "    \"\"\"Calculate document frequency for each token\"\"\"\n",
    "    return postings.map(lambda token: (token[0], len(token[1])))\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings, base_dir):\n",
    "    \"\"\"Partition and write posting lists to GCS\"\"\"\n",
    "    bucket_rdd = postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey()\n",
    "    \n",
    "    def write_bucket(b_w_pl):\n",
    "        bucket_id, word_posting_pairs = b_w_pl\n",
    "        return InvertedIndex.write_a_posting_list(\n",
    "            (bucket_id, list(word_posting_pairs)), base_dir, bucket_name\n",
    "        )\n",
    "    \n",
    "    return bucket_rdd.map(write_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Building inverted index with stemming and phrases for body text...\")\n",
    "print(\"This will take a while for the full corpus...\")\n",
    "\n",
    "# Step 1: Word counts (stemmed + phrases)\n",
    "word_counts = doc_text_pairs.flatMap(lambda x: word_count_with_phrases_stemmed(x[0], x[1]))\n",
    "\n",
    "# Step 2: Create posting lists\n",
    "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "\n",
    "# Step 3: Filter rare terms (helps reduce index size)\n",
    "MIN_DF = 5  # Minimum document frequency\n",
    "postings_filtered = postings.filter(lambda x: len(x[1]) >= MIN_DF)\n",
    "\n",
    "# Step 4: Calculate df\n",
    "w2df = calculate_df(postings_filtered)\n",
    "w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "print(f\"Total unique tokens (with df >= {MIN_DF}): {len(w2df_dict):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Write posting lists to GCS\n",
    "print(f\"Writing posting lists to {INDEX_DIR}...\")\n",
    "_ = partition_postings_and_write(postings_filtered, INDEX_DIR).collect()\n",
    "print(\"Done writing posting lists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all posting list locations\n",
    "super_posting_locs = defaultdict(list)\n",
    "\n",
    "for blob in client.list_blobs(bucket_name, prefix=INDEX_DIR):\n",
    "    if not blob.name.endswith(\"pickle\"):\n",
    "        continue\n",
    "    with blob.open(\"rb\") as f:\n",
    "        posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "print(f\"Collected posting locations for {len(super_posting_locs):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save InvertedIndex\n",
    "inverted = InvertedIndex()\n",
    "inverted.posting_locs = super_posting_locs\n",
    "inverted.df = w2df_dict\n",
    "\n",
    "# Save locally\n",
    "inverted.write_index('.', 'index')\n",
    "\n",
    "# Upload to GCS\n",
    "index_src = \"index.pkl\"\n",
    "index_dst = f'gs://{bucket_name}/body_stemmed_phrases_idx/{index_src}'\n",
    "!gsutil cp $index_src $index_dst\n",
    "\n",
    "print(f\"Index saved to {index_dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify index size\n",
    "!gsutil ls -lh $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Document Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doc_length_stemmed(doc_id, text):\n",
    "    \"\"\"Calculate document length in stemmed tokens (including phrases)\"\"\"\n",
    "    tokens = tokenize_with_phrases_stemmed(\n",
    "        text, \n",
    "        strong_phrases_broadcast.value, \n",
    "        all_stopwords_broadcast.value\n",
    "    )\n",
    "    return (doc_id, len(tokens))\n",
    "\n",
    "# Calculate lengths\n",
    "print(\"Calculating document lengths (stemmed)...\")\n",
    "doc_lengths_rdd = doc_text_pairs.map(lambda x: calc_doc_length_stemmed(x[1], x[0]))\n",
    "doc_lengths_dict = doc_lengths_rdd.collectAsMap()\n",
    "\n",
    "print(f\"Calculated lengths for {len(doc_lengths_dict):,} documents\")\n",
    "\n",
    "# Stats\n",
    "lengths = list(doc_lengths_dict.values())\n",
    "print(f\"Average doc length: {np.mean(lengths):.1f}\")\n",
    "print(f\"Median doc length: {np.median(lengths):.1f}\")\n",
    "print(f\"Max doc length: {max(lengths):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document lengths\n",
    "lengths_filename = 'body_doc_lengths_stemmed.pickle'\n",
    "\n",
    "with open(lengths_filename, 'wb') as f:\n",
    "    pickle.dump(doc_lengths_dict, f)\n",
    "\n",
    "# Upload to GCS\n",
    "lengths_dst = f'gs://{bucket_name}/body_stemmed_phrases_idx_Phrases/{lengths_filename}'\n",
    "!gsutil cp $lengths_filename $lengths_dst\n",
    "\n",
    "print(f\"Document lengths saved to {lengths_dst}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files in body_stemmed_phrases_idx/:\")\n",
    "!gsutil ls -lh gs://$bucket_name/body_stemmed_phrases_idx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPosting list files in body_stemmed_phrases/:\")\n",
    "!gsutil ls gs://$bucket_name/body_stemmed_phrases/ | head -20\n",
    "print(\"...\")\n",
    "!gsutil ls gs://$bucket_name/body_stemmed_phrases/ | wc -l\n",
    "print(\"total files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Files Created:\n",
    "\n",
    "| File | Location | Description |\n",
    "|------|----------|-------------|\n",
    "| `index.pkl` | `body_stemmed_phrases_idx/` | Inverted index (posting locs + df) |\n",
    "| `body_doc_lengths_stemmed.pickle` | `body_stemmed_phrases_idx/` | Document lengths for BM25 |\n",
    "| `strong_phrases_body_stemmed.pkl` | `body_stemmed_phrases_idx/` | Set of detected phrases (stemmed) |\n",
    "| `*.bin` | `body_stemmed_phrases/` | Binary posting list files |\n",
    "\n",
    "## Usage in search_frontend.py:\n",
    "\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "# Load phrases\n",
    "with open('strong_phrases_body_stemmed.pkl', 'rb') as f:\n",
    "    strong_phrases = pickle.load(f)\n",
    "\n",
    "# Process query with same tokenization (stemming + phrases)\n",
    "def process_query(query):\n",
    "    return tokenize_with_phrases_stemmed(query, strong_phrases, all_stopwords)\n",
    "```\n",
    "\n",
    "## Important Notes:\n",
    "- **Query must be processed the same way!** Use stemming + phrase detection\n",
    "- Stemmed tokens like `unit_state` will match queries like \"United States\", \"united states\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Body text index with STEMMING and phrases creation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
